{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DENOISING AUTOENCODER FOR MIDI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnjYUqvzBeSz"
   },
   "source": [
    "### **1. Load Libraries**\n",
    "\n",
    "The imported modules include:\n",
    "\n",
    "* `torchvision`: contains many popular computer vision datasets, deep neural network architectures, and image processing modules. We will use this to download the CIFAR10 dataset.\n",
    "* `torch.nn`: contains the deep learning neural network layers such as `Linear()`, and `Conv2d()`.\n",
    "* `transforms`: will help in defining the image transforms and normalizations.\n",
    "* `optim`: contains the deep learning optimizer classes such as `MSELoss()` and many others as well.\n",
    "* `functional`: we will use this for activation functions such as ReLU.\n",
    "* `DataLoader`: eases the task of making iterable training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCnwb04BBeSz"
   },
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "from torchinfo import summary\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "# other\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dark theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUu40u7OBeS0"
   },
   "source": [
    "### **2. Define Constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kqgrIS7BeS0"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 32\n",
    "NOISE_FACTOR = 0.0\n",
    "NUM_PERMUTATIONS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4Xhh5FSBeS1"
   },
   "source": [
    "### **3. Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_plot(a1, a2, t1=\"noisy\", t2=\"denoised\", set_axis=\"off\", video=False):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.title(t1)\n",
    "    plt.imshow(\n",
    "        np.squeeze(a1),\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        cmap=\"magma\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.axis(set_axis)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.title(t2)\n",
    "    plt.imshow(\n",
    "        np.squeeze(a2),\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        cmap=\"magma\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.axis(set_axis)\n",
    "\n",
    "    if video:\n",
    "        dirname = f\"video\"\n",
    "        if not os.path.isdir(dirname):\n",
    "            os.mkdir(dirname)\n",
    "        plt.savefig(\n",
    "            f\"{dirname}/plot_{datetime.now().strftime('%y-%m-%d_%H%M%S')}.png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVfTQ91T0NXa"
   },
   "outputs": [],
   "source": [
    "def format_image(image, remove_time=False):\n",
    "    if remove_time:\n",
    "        image = np.delete(image, 0, axis=1)\n",
    "    image = torch.from_numpy(np.expand_dims(image, 0)).to(torch.float32)\n",
    "    if torch.any(image > 1.0):\n",
    "        image = image / image.max()\n",
    "    image = F.pad(input=image, pad=(0, 12, 1, 1), mode=\"constant\", value=0.0)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmmU_NufBeS1"
   },
   "source": [
    "### **4. Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_images = np.load(\"data/all_loops.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_image_vertically(name, array, num_iterations):\n",
    "    shifted_images = []\n",
    "\n",
    "    def find_non_zero_bounds(arr):\n",
    "        # Find the first and last row index with a non-zero element\n",
    "        rows_with_non_zero = np.where(arr.any(axis=1))[0]\n",
    "        if len(rows_with_non_zero) == 0:\n",
    "            return (0, arr.shape[0] - 1)  # Handle case with no non-zero elements\n",
    "        return rows_with_non_zero[0], rows_with_non_zero[-1]\n",
    "\n",
    "    def shift_array(arr, up=0, down=0):\n",
    "        # Shift array vertically\n",
    "        if up > 0:\n",
    "            arr = np.roll(arr, -up, axis=0)\n",
    "            arr[-up:] = 0\n",
    "        elif down > 0:\n",
    "            arr = np.roll(arr, down, axis=0)\n",
    "            arr[:down] = 0\n",
    "        return arr\n",
    "\n",
    "    highest, lowest = find_non_zero_bounds(array)\n",
    "    maximum_up = highest\n",
    "    maximum_down = array.shape[0] - lowest - 1\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # Shift up and then down, decreasing the shift amount in each iteration\n",
    "        for i in range(maximum_up, 0, -1):\n",
    "            new_key = f\"{Path(name).stem}_u{i:02d}\"\n",
    "            shifted_images.append((new_key, np.copy(shift_array(array, up=i))))\n",
    "        for i in range(maximum_down, 0, -1):\n",
    "            new_key = f\"{Path(name).stem}_d{i:02d}\"\n",
    "            shifted_images.append((new_key, np.copy(shift_array(array, down=i))))\n",
    "\n",
    "    random.shuffle(shifted_images)\n",
    "\n",
    "    return shifted_images[:num_iterations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SY5xHsFuBeS0"
   },
   "source": [
    "### **5. Prepare the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(clean_images, num_permutations=NUM_PERMUTATIONS, vshift=True):\n",
    "    \"\"\"Augments a set of passed-in images by a factor of 2*num_permutations\"\"\"\n",
    "    shifted_images = []\n",
    "    noisy_images = []\n",
    "\n",
    "    for name, image in tqdm(\n",
    "        list(clean_images.items()), unit=\"images\", dynamic_ncols=True\n",
    "    ):\n",
    "        time_factor = image[:, 0]  # save time factor\n",
    "        image = np.delete(image, 0, axis=1)  # remove it from the image though\n",
    "        if vshift:\n",
    "            # vertical shift images\n",
    "            shifted_images.append(shift_image_vertically(name, image, num_permutations))\n",
    "        else:\n",
    "            # reformat clean image array\n",
    "            shifted_images.append([(name, image)])\n",
    "\n",
    "        # add noise to images\n",
    "        for si in shifted_images[-1]:\n",
    "            new_key, shifted_image = si\n",
    "            for _ in range(num_permutations):\n",
    "                # normalize\n",
    "                noisy_image = shifted_image / np.max(shifted_image)\n",
    "\n",
    "                # corrupt\n",
    "                noisy_image = torch.from_numpy(\n",
    "                    noisy_image\n",
    "                ) + NOISE_FACTOR * torch.randn(noisy_image.shape)\n",
    "\n",
    "                # reformat\n",
    "                noisy_image = format_image(noisy_image)\n",
    "\n",
    "                noisy_images.append((new_key, noisy_image))\n",
    "\n",
    "    random.shuffle(noisy_images)\n",
    "\n",
    "    return shifted_images, noisy_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation\n",
    "**WARNING: This may use a ton of memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_images, training_data = augment_data(clean_images, vshift=True)\n",
    "\n",
    "# output_file = f\"augmented_data_{datetime.now().strftime('%y-%m-%d_%H%M%S')}\"\n",
    "# np.savez_compressed(\n",
    "#     os.path.join(\"data\", output_file),\n",
    "#     **{name: arr for name, arr in noisy_images},\n",
    "# )\n",
    "print(\n",
    "    f\"used {len(list(clean_images.keys()))} clean images to generate {len(training_data)} noisy images of shape {training_data[0][1].size()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in training_data[:2]:\n",
    "    k, v = data\n",
    "    compare_plot(\n",
    "        clean_images[k[: k.rfind(\"_\")] + \".mid\"], v, k, f\"noise ({NOISE_FACTOR})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIDILoopsDataset(Dataset):\n",
    "    def __init__(self, image_paths, transforms=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = read_image(image_path)\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yl-Xy9STBeS1"
   },
   "source": [
    "### **6. Define the AutoEncoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model, a=0.0, b=1.0):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.ConvTranspose2d):\n",
    "            # Apply uniform initialization to the weights\n",
    "            nn.init.uniform_(module.weight, a, b)\n",
    "            # Check if the module has a bias attribute\n",
    "            if module.bias is not None:\n",
    "                # Initialize the bias with the same bounds\n",
    "                nn.init.uniform_(module.bias, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bad AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aK6hMJrjBeS2"
   },
   "outputs": [],
   "source": [
    "class BadAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BadAutoEncoder, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.enc1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.enc2 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.enc3 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.enc4 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.dec1 = nn.ConvTranspose2d(8, 8, kernel_size=3, stride=2)\n",
    "        self.dec2 = nn.ConvTranspose2d(8, 16, kernel_size=3, stride=2)\n",
    "        self.dec3 = nn.ConvTranspose2d(16, 32, kernel_size=2, stride=2)\n",
    "        self.dec4 = nn.ConvTranspose2d(32, 64, kernel_size=2, stride=2)\n",
    "        self.out = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.pool(F.silu(self.enc1(x)))\n",
    "        x = self.pool(F.silu(self.enc2(x)))\n",
    "        x = self.pool(F.silu(self.enc3(x)))\n",
    "        x = self.pool(F.silu(self.enc4(x)))  # latent space representation\n",
    "\n",
    "        # Decoder\n",
    "        x = F.silu(self.dec1(x))\n",
    "        x = F.silu(self.dec2(x))\n",
    "        x = F.silu(self.dec3(x))\n",
    "        x = F.silu(self.dec4(x))\n",
    "        # x = torch.sigmoid(self.out(x))\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = BadAutoEncoder().to(device)\n",
    "initialize_weights(model, 0.01, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep AutoEncoder\n",
    "\n",
    "From the [Lightning tutorial](https://colab.research.google.com/github/PytorchLightning/lightning-tutorials/blob/publication/.notebooks/course_UvA-DL/08-deep-autoencoders.ipynb#scrollTo=cf346a22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input_channels: int,\n",
    "        base_channel_size: int,\n",
    "        latent_dim: int,\n",
    "        act_fn = nn.GELU,\n",
    "    ):\n",
    "        \"\"\"Encoder.\n",
    "\n",
    "        Args:\n",
    "           num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                num_input_channels, c_hid, kernel_size=3, padding=1, stride=2\n",
    "            ),  # 32x32 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(\n",
    "                c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2\n",
    "            ),  # 16x16 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(\n",
    "                2 * c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2\n",
    "            ),  # 8x8 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(),  # Image grid to single feature vector\n",
    "            nn.Linear(2 * 16 * c_hid, latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input_channels: int,\n",
    "        base_channel_size: int,\n",
    "        latent_dim: int,\n",
    "        act_fn = nn.GELU,\n",
    "    ):\n",
    "        \"\"\"Decoder.\n",
    "\n",
    "        Args:\n",
    "           num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(nn.Linear(latent_dim, 2 * 16 * c_hid), act_fn())\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                2 * c_hid,\n",
    "                2 * c_hid,\n",
    "                kernel_size=3,\n",
    "                output_padding=1,\n",
    "                padding=1,\n",
    "                stride=2,\n",
    "            ),  # 4x4 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(\n",
    "                2 * c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2\n",
    "            ),  # 8x8 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(\n",
    "                c_hid,\n",
    "                num_input_channels,\n",
    "                kernel_size=3,\n",
    "                output_padding=1,\n",
    "                padding=1,\n",
    "                stride=2,\n",
    "            ),  # 16x16 => 32x32\n",
    "            nn.Tanh(),  # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Autoencoder(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_channel_size: int,\n",
    "        latent_dim: int,\n",
    "        encoder_class = Encoder,\n",
    "        decoder_class = Decoder,\n",
    "        num_input_channels: int = 3,\n",
    "        width: int = 32,\n",
    "        height: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters of autoencoder\n",
    "        self.save_hyperparameters()\n",
    "        # Creating encoder and decoder\n",
    "        self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        # Example input array needed for visualizing the graph of the network\n",
    "        self.example_input_array = torch.zeros(2, num_input_channels, width, height)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward function takes in an image and returns the reconstructed image.\"\"\"\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def _get_reconstruction_loss(self, batch):\n",
    "        \"\"\"Given a batch of images, this function returns the reconstruction loss (MSE in our case).\"\"\"\n",
    "        x, _ = batch  # We do not need the labels\n",
    "        x_hat = self.forward(x)\n",
    "        loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "        loss = loss.sum(dim=[1, 2, 3]).mean(dim=[0])\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # Using a scheduler is optional but can be helpful.\n",
    "        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.2, patience=20, min_lr=5e-5\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"test_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nVMS7OsyBeS2",
    "outputId": "305cb08f-e595-4539-cb3c-47327bfbb0d7"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQEvArOFBeS2",
    "outputId": "424eb883-28e5-4bd0-aa5b-d658ed2e35e4"
   },
   "outputs": [],
   "source": [
    "summary(model, (1, 58, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjjIFa2yBeS2"
   },
   "source": [
    "### **7. Optimizer and Loss Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiUmiKepBeS2"
   },
   "outputs": [],
   "source": [
    "# the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "# the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIUfa004BeS2"
   },
   "source": [
    "### **8. Train**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "8GnlBhY1HiwV",
    "outputId": "05ebe3cf-41fa-469d-fcc0-3f5c7a3cb160"
   },
   "outputs": [],
   "source": [
    "def train(net: nn.Module, training_data, epochs=NUM_EPOCHS, plot=False, video=False, tb=False):\n",
    "    train_loss = []\n",
    "    if tb:\n",
    "        writer = SummaryWriter(f\"runs/{datetime.now().strftime('%y-%m-%d_%H%M%S')}\")\n",
    "    for epoch in trange(epochs, desc=\"Total \"):\n",
    "        running_loss = 0.0\n",
    "        with tqdm(training_data, unit=\"images\", dynamic_ncols=False) as tepoch:\n",
    "            for i, (name, input_image) in enumerate(tepoch):\n",
    "                tepoch.set_description(f\"Epoch {epoch + 1}\")\n",
    "\n",
    "                # train\n",
    "                input_image = input_image.to(device)  # (1, 60, 412)\n",
    "                optimizer.zero_grad()\n",
    "                predicted_image = net(input_image)  # compute prediction\n",
    "                loss = loss_fn(predicted_image, input_image)  # compute loss\n",
    "                loss.backward()  # backprop\n",
    "                optimizer.step()  # update parameters\n",
    "\n",
    "                if plot or video:\n",
    "                    compare_plot(\n",
    "                        input_image.cpu().data,\n",
    "                        predicted_image.cpu().data,\n",
    "                        name,\n",
    "                        f\"output (loss={loss})\",\n",
    "                        video=video,\n",
    "                    )\n",
    "\n",
    "                # tensorboard logging\n",
    "                if tb:\n",
    "                    global_step = epoch * len(training_data) + i\n",
    "                    writer.add_scalar(\"training/loss\", loss.item(), global_step)\n",
    "                    for p_name, param in model.named_parameters():\n",
    "                        writer.add_histogram(\n",
    "                            f\"weights/{p_name}\", param.data, global_step\n",
    "                        )\n",
    "                        if param.requires_grad:\n",
    "                            writer.add_histogram(\n",
    "                                f\"gradients/{p_name}.grad\", param.grad, global_step\n",
    "                            )\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=f\"{loss:.02f}\")\n",
    "\n",
    "            loss = running_loss / len(training_data)\n",
    "            train_loss.append(loss)\n",
    "\n",
    "    if tb:\n",
    "        writer.close()\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = train(model, training_data, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_loss)\n",
    "plt.title(\"Train Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_noisy = training_data[0][1] + NOISE_FACTOR * torch.randn(training_data[0][1].shape)\n",
    "img_noisy = img_noisy.to(device)\n",
    "outputs = model(img_noisy)\n",
    "\n",
    "compare_plot(img_noisy.cpu().data, outputs.cpu().data, training_data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_tensor = [\n",
    "    (k, torch.from_numpy(np.expand_dims(format_image(v, True), 0)).to(torch.float32))\n",
    "    for k, v in list(clean_images.items())\n",
    "]\n",
    "random.shuffle(si_tensor)\n",
    "overfit_set = [si_tensor[0]] * 1000\n",
    "\n",
    "initialize_weights(model, 0.01, 0.1)\n",
    "train(model, overfit_set, epochs=3, tb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_noisy = overfit_set[0][1] + NOISE_FACTOR * torch.randn(overfit_set[0][1].shape)\n",
    "img_noisy = img_noisy.to(device)\n",
    "outputs = model(img_noisy)\n",
    "\n",
    "compare_plot(img_noisy.cpu().data, outputs.cpu().data, overfit_set[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import shutil\n",
    "\n",
    "image_folder = \"video\"\n",
    "video_name = \"ugh.mp4\"\n",
    "fps = 20\n",
    "\n",
    "images = [img for img in os.listdir(image_folder) if img.endswith(\".png\")]\n",
    "images.sort()  # Sort the images by name\n",
    "\n",
    "# Determine the width and height from the first image\n",
    "frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "height, width, layers = frame.shape\n",
    "\n",
    "video = cv2.VideoWriter(\n",
    "    video_name, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height)\n",
    ")\n",
    "\n",
    "for image in images:\n",
    "    video.write(cv2.imread(os.path.join(image_folder, image)))\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()\n",
    "shutil.rmtree(image_folder)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "interpreter": {
   "hash": "884a8f81666a19c0851426c83cd6eaa7b212468ad852fb3caa21591c98d4369f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
